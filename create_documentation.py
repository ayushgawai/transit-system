#!/usr/bin/env python3
"""
Create IEEE Format Documentation for Transit System
"""
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement
import os

def add_page_break(doc):
    """Add a page break"""
    doc.add_page_break()

def add_heading_with_style(doc, text, level=1):
    """Add heading with proper formatting"""
    heading = doc.add_heading(text, level=level)
    return heading

def add_paragraph_with_style(doc, text, bold=False, italic=False):
    """Add paragraph with optional formatting"""
    para = doc.add_paragraph(text)
    if bold:
        for run in para.runs:
            run.bold = True
    if italic:
        for run in para.runs:
            run.italic = True
    return para

def add_table_placeholder(doc, title, description):
    """Add a placeholder for a table"""
    para = doc.add_paragraph()
    run = para.add_run(f"[TABLE PLACEHOLDER: {title}]")
    run.bold = True
    run.font.color.rgb = RGBColor(255, 0, 0)
    doc.add_paragraph(description, style='Intense Quote')
    doc.add_paragraph()

def add_image_placeholder(doc, title, description, prompt=""):
    """Add a placeholder for an image"""
    para = doc.add_paragraph()
    run = para.add_run(f"[IMAGE PLACEHOLDER: {title}]")
    run.bold = True
    run.font.color.rgb = RGBColor(0, 0, 255)
    doc.add_paragraph(description, style='Intense Quote')
    if prompt:
        doc.add_paragraph(f"Image Generation Prompt: {prompt}", style='Intense Quote')
    doc.add_paragraph()

def create_documentation():
    """Create comprehensive IEEE format documentation"""
    
    doc = Document()
    
    # Set document margins
    sections = doc.sections
    for section in sections:
        section.top_margin = Inches(1)
        section.bottom_margin = Inches(1)
        section.left_margin = Inches(1)
        section.right_margin = Inches(1)
    
    # Title Page
    title_para = doc.add_paragraph()
    title_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    title_run = title_para.add_run("Transit Service Reliability & Demand Planning System")
    title_run.font.size = Pt(24)
    title_run.bold = True
    
    doc.add_paragraph()
    
    subtitle_para = doc.add_paragraph()
    subtitle_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    subtitle_run = subtitle_para.add_run("A Production-Capable Prototype for Metropolitan Transit Operators")
    subtitle_run.font.size = Pt(16)
    subtitle_run.italic = True
    
    doc.add_paragraph()
    doc.add_paragraph()
    
    author_para = doc.add_paragraph()
    author_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    author_run = author_para.add_run("Ayush Gawai")
    author_run.font.size = Pt(14)
    
    doc.add_paragraph()
    
    org_para = doc.add_paragraph()
    org_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    org_run = org_para.add_run("San Jose State University")
    org_run.font.size = Pt(12)
    
    doc.add_paragraph()
    
    date_para = doc.add_paragraph()
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    date_run = date_para.add_run("December 2025")
    date_run.font.size = Pt(12)
    
    add_page_break(doc)
    
    # Abstract
    add_heading_with_style(doc, "Abstract", 1)
    abstract_text = """This document presents a comprehensive Transit Service Reliability & Demand Planning System, 
    a production-capable prototype designed for metropolitan transit operators. The system transforms raw real-time 
    transit data into actionable intelligence for operations teams, enabling data-driven decision-making for service 
    optimization, resource allocation, and passenger experience improvement. The architecture leverages modern cloud 
    technologies including Apache Airflow for orchestration, dbt for data transformation, Snowflake as the data 
    warehouse, AWS Secrets Manager for secure credential management, and a React-based frontend for interactive 
    visualization. The system processes both static GTFS (General Transit Feed Specification) data and real-time 
    streaming data from TransitApp API, providing explainable reliability KPIs, AI-powered recommendations, and 
    ML-based demand forecasting. This documentation provides a complete technical overview, setup instructions, 
    architecture details, and operational guidelines for engineers and stakeholders."""
    doc.add_paragraph(abstract_text)
    doc.add_paragraph()
    
    # Table of Contents placeholder
    add_heading_with_style(doc, "Table of Contents", 1)
    doc.add_paragraph("[TABLE OF CONTENTS WILL BE AUTO-GENERATED BY WORD]")
    add_page_break(doc)
    
    # 1. Introduction
    add_heading_with_style(doc, "1. Introduction", 1)
    
    add_heading_with_style(doc, "1.1 Purpose of the Project", 2)
    doc.add_paragraph("""The Transit Service Reliability & Demand Planning System addresses critical challenges 
    faced by metropolitan transit operators in managing service reliability, optimizing resource allocation, and 
    improving passenger experience. Traditional transit management systems often lack real-time analytics capabilities, 
    making it difficult for operations teams to identify service issues, predict demand patterns, and make informed 
    decisions about route optimization and capacity planning.""")
    
    doc.add_paragraph("""This system provides a comprehensive solution that integrates multiple data sources, 
    processes them through a modern data pipeline, and presents actionable insights through an intuitive dashboard. 
    The system is designed to be production-ready, scalable, and maintainable, following industry best practices 
    for data engineering and software development.""")
    
    add_heading_with_style(doc, "1.2 Why We Chose This Project", 2)
    doc.add_paragraph("""This project was selected for several compelling reasons:""")
    
    reasons = [
        ("Real-World Impact", "Transit systems are critical infrastructure that directly affects millions of people daily. Improving transit reliability and efficiency has tangible benefits for communities, reducing commute times, environmental impact, and economic costs."),
        ("Technical Complexity", "The project requires integration of multiple technologies (streaming data, batch processing, ML, cloud services) providing comprehensive learning opportunities in modern data engineering."),
        ("Scalability Challenge", "Transit systems generate massive amounts of data. Building a system that can handle real-time streaming data, historical analysis, and predictive modeling demonstrates advanced engineering skills."),
        ("Industry Relevance", "The technologies used (Airflow, dbt, Snowflake, AWS, React) are industry-standard tools, making this project directly applicable to real-world data engineering roles."),
        ("End-to-End Solution", "Unlike many academic projects, this system provides a complete solution from data ingestion to visualization, demonstrating full-stack data engineering capabilities."),
        ("Production-Ready Design", "The system is designed with production considerations including error handling, monitoring, security, and maintainability, preparing for real-world deployment.")
    ]
    
    for title, description in reasons:
        para = doc.add_paragraph()
        run = para.add_run(f"{title}: ")
        run.bold = True
        para.add_run(description)
        doc.add_paragraph()
    
    add_heading_with_style(doc, "1.3 Problem Statement", 2)
    doc.add_paragraph("""Metropolitan transit operators face significant challenges in managing service reliability 
    and optimizing operations:""")
    
    problems = [
        "Lack of real-time visibility into service performance and delays",
        "Difficulty in identifying problematic routes, stops, or time periods",
        "Insufficient data-driven insights for resource allocation decisions",
        "Limited ability to predict demand patterns and optimize schedules",
        "Fragmented data sources (GTFS feeds, real-time APIs) without unified analytics",
        "Manual analysis processes that are time-consuming and error-prone",
        "Inability to provide explainable metrics and recommendations to operations teams"
    ]
    
    for problem in problems:
        doc.add_paragraph(problem, style='List Bullet')
    
    doc.add_paragraph("""The Transit Service Reliability & Demand Planning System addresses these challenges by 
    providing a unified platform that ingests, processes, and analyzes transit data to deliver actionable insights 
    in real-time.""")
    
    add_page_break(doc)
    
    # 2. System Architecture
    add_heading_with_style(doc, "2. System Architecture", 1)
    
    add_heading_with_style(doc, "2.1 High-Level Architecture", 2)
    doc.add_paragraph("""The system follows a modern, cloud-native architecture with clear separation of concerns 
    across data ingestion, processing, storage, and presentation layers.""")
    
    add_image_placeholder(
        doc,
        "High-Level System Architecture Diagram",
        "A comprehensive diagram showing all system components, data flows, and interactions. Should include: Data Sources (GTFS, TransitApp API), Ingestion Layer (Airflow DAGs, Kafka), Processing Layer (dbt transformations), Storage Layer (Snowflake), ML Layer (Snowflake ML), and Presentation Layer (React Frontend, FastAPI Backend).",
        "Create a high-level architecture diagram showing: 1) Data sources (GTFS feeds, TransitApp API) on the left, 2) Ingestion layer (Airflow, Kafka) in the middle-left, 3) Processing layer (dbt) in the center, 4) Storage layer (Snowflake with schemas: LANDING, RAW, ANALYTICS, ML) in the middle-right, 5) ML forecasting layer above storage, 6) Backend API (FastAPI) connecting to Snowflake, 7) Frontend (React) on the right. Use arrows to show data flow direction. Include AWS Secrets Manager as a security component. Use professional colors and clear labels."
    )
    
    add_heading_with_style(doc, "2.2 Data Flow Architecture", 2)
    doc.add_paragraph("""The system processes data through multiple layers:""")
    
    data_flow_steps = [
        ("Landing Layer", "Raw data from GTFS feeds and TransitApp API is stored in landing tables without transformation. This preserves original data for audit and reprocessing."),
        ("Raw Layer (Staging)", "dbt models transform landing data into clean, standardized staging tables (stg_gtfs_stops, stg_gtfs_routes, stg_gtfs_trips, stg_gtfs_stop_times)."),
        ("Transform Layer", "Intermediate transformations combine data from multiple sources (e.g., route_departures combines stops, routes, and trips)."),
        ("Analytics Layer", "Business metrics are calculated (reliability_metrics, demand_metrics, crowding_metrics, revenue_metrics, decision_support)."),
        ("ML Layer", "Machine learning models generate forecasts for demand and delays using historical patterns.")
    ]
    
    for step, description in data_flow_steps:
        para = doc.add_paragraph()
        run = para.add_run(f"{step}: ")
        run.bold = True
        para.add_run(description)
        doc.add_paragraph()
    
    add_image_placeholder(
        doc,
        "Data Flow Diagram",
        "A detailed data flow diagram showing the progression from raw data sources through each layer (Landing → Raw → Transform → Analytics → ML) with specific table names and transformation types.",
        "Create a data flow diagram showing: 1) GTFS ZIP files and TransitApp API as sources, 2) Landing tables (LANDING_GTFS_STOPS, LANDING_STREAMING_DEPARTURES), 3) Raw/Staging tables (STG_GTFS_STOPS, STG_GTFS_ROUTES, etc.), 4) Transform tables (ROUTE_DEPARTURES), 5) Analytics tables (RELIABILITY_METRICS, DEMAND_METRICS, etc.), 6) ML tables (DEMAND_FORECAST, DELAY_FORECAST). Show dbt transformations between layers. Use different colors for each layer."
    )
    
    add_heading_with_style(doc, "2.3 Component Architecture", 2)
    
    components = [
        ("Data Ingestion", "Airflow DAGs orchestrate data extraction from GTFS feeds and TransitApp API. Kafka handles real-time streaming data ingestion."),
        ("Data Transformation", "dbt (data build tool) performs all data transformations using SQL-based models, ensuring version control and reproducibility."),
        ("Data Storage", "Snowflake serves as the centralized data warehouse, storing data in multiple schemas (LANDING, RAW, ANALYTICS, ML) for different purposes."),
        ("Machine Learning", "Snowflake ML provides forecasting capabilities for demand prediction and delay forecasting."),
        ("API Layer", "FastAPI backend serves data to the frontend, handling authentication, query optimization, and response formatting."),
        ("Presentation Layer", "React-based frontend provides interactive dashboards, analytics views, map visualizations, and AI-powered recommendations.")
    ]
    
    for component, description in components:
        para = doc.add_paragraph()
        run = para.add_run(f"{component}: ")
        run.bold = True
        para.add_run(description)
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 3. Technology Stack
    add_heading_with_style(doc, "3. Technology Stack and Implementation", 1)
    
    technologies = [
        {
            "name": "Apache Airflow",
            "purpose": "Workflow Orchestration",
            "description": "Apache Airflow is used to orchestrate all data pipeline workflows. It manages DAGs (Directed Acyclic Graphs) that define data extraction, transformation, and loading processes.",
            "implementation": "Three main DAGs are configured: (1) gtfs_incremental_dag - Extracts GTFS data from public feeds (BART, VTA) every 6 hours, (2) streaming_dag - Manages real-time data ingestion via Kafka, (3) ml_forecast_dag - Triggers ML forecasting models. Airflow runs in Docker containers for local development and can be deployed to AWS ECS/EKS for production.",
            "technical_details": "DAGs are defined in Python using Airflow's TaskFlow API. Tasks include Python operators for data extraction, Bash operators for dbt execution, and sensors for dependency management. Airflow's scheduler ensures tasks run on schedule and handles retries on failure."
        },
        {
            "name": "dbt (data build tool)",
            "purpose": "Data Transformation",
            "description": "dbt handles all data transformations using SQL-based models. It provides version control, testing, and documentation for data transformations.",
            "implementation": "dbt models are organized into layers: (1) landing_to_raw - Transforms raw GTFS data into staging tables, (2) streaming_to_analytics - Processes streaming data, (3) transform - Intermediate transformations, (4) analytics - Business metrics calculation, (5) ml_forecasts - ML model outputs. Models use incremental materialization for efficiency.",
            "technical_details": "dbt connects to Snowflake using profiles.yml configuration. Models use Jinja templating for dynamic SQL generation. Incremental models use merge strategy to avoid duplicates. dbt_utils package provides additional macros for data quality checks."
        },
        {
            "name": "Snowflake",
            "purpose": "Data Warehouse",
            "description": "Snowflake serves as the centralized data warehouse, providing scalable storage and compute for all transit data.",
            "implementation": "Data is organized into schemas: (1) LANDING - Raw ingested data, (2) ANALYTICS - Staging and analytics tables (STG_GTFS_*, RELIABILITY_METRICS, DEMAND_METRICS, etc.), (3) ML - Machine learning forecast tables. Snowflake's columnar storage and automatic scaling handle varying query loads efficiently.",
            "technical_details": "Connection uses Snowflake connector with credentials from AWS Secrets Manager. Tables use appropriate data types (VARCHAR, INTEGER, FLOAT, TIMESTAMP_NTZ). Incremental loading uses MERGE statements. Snowflake ML functions are used for forecasting."
        },
        {
            "name": "AWS Secrets Manager",
            "purpose": "Credential Management",
            "description": "AWS Secrets Manager securely stores all sensitive credentials including Snowflake connection details, API keys, and database passwords.",
            "implementation": "Secrets are stored with naming convention: transit/{service}-{environment} (e.g., transit/snowflake-dev, transit/transitapp-api-dev, transit/perplexity-api-dev). The application loads secrets at runtime using boto3, eliminating hardcoded credentials.",
            "technical_details": "Python code uses boto3 to retrieve secrets. Secrets can be stored as JSON objects or plaintext strings. The system falls back to environment variables for local development. IAM roles control access to secrets."
        },
        {
            "name": "Kafka (Apache Kafka)",
            "purpose": "Real-Time Streaming",
            "description": "Kafka handles real-time data streaming from TransitApp API, providing a buffer and ensuring reliable data delivery.",
            "implementation": "Kafka producer (transit_streaming_producer.py) fetches data from TransitApp API and publishes to 'transit-streaming' topic. Kafka consumer (kafka_consumer_to_landing.py) consumes messages and writes to Snowflake landing tables. Kafka runs in Docker containers locally.",
            "technical_details": "Kafka uses topic-based messaging. Producer respects API rate limits (1 call per 5 minutes). Consumer uses consumer groups for parallel processing. Messages are in JSON format. Kafka ensures at-least-once delivery semantics."
        },
        {
            "name": "FastAPI (Backend)",
            "purpose": "REST API",
            "description": "FastAPI provides the backend API layer, serving data to the React frontend with optimized queries and response formatting.",
            "implementation": "API endpoints include: /api/kpis - Key performance indicators, /api/routes - Route details, /api/stops - Stop information, /api/analytics/* - Analytics endpoints, /api/admin/status - System status, /api/chat - LLM-powered chat. API uses async/await for performance.",
            "technical_details": "FastAPI connects to Snowflake using connection pooling. Endpoints use Pydantic models for request/response validation. CORS middleware enables frontend communication. API documentation is auto-generated at /docs endpoint."
        },
        {
            "name": "React (Frontend)",
            "purpose": "User Interface",
            "description": "React provides the interactive frontend dashboard with real-time data visualization, charts, maps, and AI recommendations.",
            "implementation": "Frontend pages include: Dashboard - Overview metrics and charts, Routes - Route performance details, Analytics - Detailed analytics, Map View - Geographic visualization, Forecasts - ML predictions, Admin - System status. Uses React hooks for state management and Axios for API calls.",
            "technical_details": "Built with Vite for fast development. Uses Tailwind CSS for styling. Chart.js for data visualization. Leaflet for map rendering. TypeScript for type safety. Responsive design for mobile and desktop."
        },
        {
            "name": "Perplexity AI / LLM Integration",
            "purpose": "AI-Powered Insights",
            "description": "LLM integration provides natural language querying of transit data and AI-powered recommendations for operations teams.",
            "implementation": "Chat endpoint (/api/chat) accepts natural language questions about transit data. System provides schema context to LLM for accurate SQL generation. LLM generates SQL queries, executes them safely, and returns human-readable explanations.",
            "technical_details": "Uses Perplexity AI API (can be configured for OpenAI or Gemini). System prompt includes table schemas and column definitions. SQL queries are validated before execution. Responses include both data and explanations."
        },
        {
            "name": "Docker & Docker Compose",
            "purpose": "Containerization",
            "description": "Docker containers package all services for consistent deployment across environments.",
            "implementation": "docker-compose.local.yml defines services: Airflow (webserver + scheduler), Kafka, Zookeeper. Each service has its own Dockerfile. Volumes mount code for development. Networks enable service communication.",
            "technical_details": "Airflow uses official apache/airflow image with custom requirements. Kafka uses confluentinc images. Volumes persist data between restarts. Environment variables configure services."
        }
    ]
    
    for i, tech in enumerate(technologies, 1):
        add_heading_with_style(doc, f"3.{i} {tech['name']}", 2)
        para = doc.add_paragraph()
        run = para.add_run("Purpose: ")
        run.bold = True
        para.add_run(tech['purpose'])
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Description: ")
        run.bold = True
        para.add_run(tech['description'])
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Implementation: ")
        run.bold = True
        para.add_run(tech['implementation'])
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Technical Details: ")
        run.bold = True
        para.add_run(tech['technical_details'])
        doc.add_paragraph()
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 4. Database Schema and Tables
    add_heading_with_style(doc, "4. Database Schema and Tables", 1)
    
    doc.add_paragraph("""The system uses Snowflake as the data warehouse with the following database and schema structure:""")
    doc.add_paragraph()
    
    para = doc.add_paragraph()
    run = para.add_run("Database: ")
    run.bold = True
    para.add_run("USER_DB_HORNET")
    doc.add_paragraph()
    
    schemas_info = [
        {
            "name": "ANALYTICS",
            "description": "Primary schema containing staging tables, analytics marts, and streaming data landing tables.",
            "tables": [
                {
                    "name": "STG_GTFS_STOPS",
                    "description": "Staging table for GTFS stops data. Contains stop locations, names, and metadata.",
                    "key_columns": "STOP_ID (PK), STOP_NAME, STOP_LAT, STOP_LON, AGENCY, LOADED_AT"
                },
                {
                    "name": "STG_GTFS_ROUTES",
                    "description": "Staging table for GTFS routes. Contains route information including names, types, and agency.",
                    "key_columns": "ROUTE_ID (PK), ROUTE_SHORT_NAME, ROUTE_LONG_NAME, AGENCY, LOADED_AT"
                },
                {
                    "name": "STG_GTFS_TRIPS",
                    "description": "Staging table for GTFS trips. Links routes to service patterns and directions.",
                    "key_columns": "TRIP_ID (PK), ROUTE_ID (FK), SERVICE_ID, TRIP_HEADSIGN, LOADED_AT"
                },
                {
                    "name": "STG_GTFS_STOP_TIMES",
                    "description": "Staging table for GTFS stop times. Contains scheduled arrival/departure times for each stop on each trip.",
                    "key_columns": "TRIP_ID (FK), STOP_ID (FK), STOP_SEQUENCE, ARRIVAL_TIME, DEPARTURE_TIME, LOADED_AT"
                },
                {
                    "name": "LANDING_STREAMING_DEPARTURES",
                    "description": "Landing table for real-time streaming departures from TransitApp API. Contains live departure data with delays and real-time status.",
                    "key_columns": "ID (PK), GLOBAL_STOP_ID, STOP_NAME, GLOBAL_ROUTE_ID, ROUTE_SHORT_NAME, ROUTE_LONG_NAME, AGENCY, CITY, SCHEDULED_DEPARTURE_TIME, DEPARTURE_TIME, IS_REAL_TIME, DELAY_SECONDS, CONSUMED_AT"
                },
                {
                    "name": "RELIABILITY_METRICS",
                    "description": "Analytics mart calculating service reliability metrics including on-time performance, headway gaps, and reliability scores by route and hour.",
                    "key_columns": "ID (PK), ROUTE_GLOBAL_ID, ROUTE_SHORT_NAME, DEPARTURE_DATE, DEPARTURE_HOUR, TOTAL_DEPARTURES, ON_TIME_DEPARTURES, AVG_DELAY_SECONDS, RELIABILITY_SCORE, UPDATED_AT"
                },
                {
                    "name": "DEMAND_METRICS",
                    "description": "Analytics mart calculating demand patterns by stop, route, and time period. Includes peak period analysis and demand intensity scores.",
                    "key_columns": "ID (PK), STOP_GLOBAL_ID, ROUTE_GLOBAL_ID, DEPARTURE_DATE, TOTAL_DEPARTURES, AM_PEAK_DEPARTURES, PM_PEAK_DEPARTURES, DEMAND_INTENSITY_SCORE, UPDATED_AT"
                },
                {
                    "name": "CROWDING_METRICS",
                    "description": "Analytics mart for capacity and crowding analysis. Calculates utilization rates and identifies overcrowded routes/stops.",
                    "key_columns": "ID (PK), ROUTE_GLOBAL_ID, STOP_GLOBAL_ID, DEPARTURE_DATE, DEPARTURE_HOUR, CAPACITY_UTILIZATION, CROWDING_SCORE, UPDATED_AT"
                },
                {
                    "name": "REVENUE_METRICS",
                    "description": "Analytics mart for revenue analysis. Calculates estimated revenue by route, stop, and time period based on fare data and ridership.",
                    "key_columns": "ID (PK), ROUTE_GLOBAL_ID, STOP_GLOBAL_ID, DEPARTURE_DATE, ESTIMATED_REVENUE, RIDERSHIP_COUNT, UPDATED_AT"
                },
                {
                    "name": "DECISION_SUPPORT",
                    "description": "Analytics mart providing decision support metrics. Combines reliability, demand, and revenue metrics to provide actionable insights.",
                    "key_columns": "ID (PK), ROUTE_GLOBAL_ID, METRIC_TYPE, METRIC_VALUE, SEVERITY, RECOMMENDATION, UPDATED_AT"
                }
            ]
        },
        {
            "name": "ML",
            "description": "Schema containing machine learning forecast tables generated by Snowflake ML models.",
            "tables": [
                {
                    "name": "DEMAND_FORECAST",
                    "description": "ML forecast table predicting future demand (departure counts) by route. Uses historical patterns to predict next 7 days.",
                    "key_columns": "ROUTE_ID, ROUTE_SHORT_NAME, AGENCY, FORECAST_DATE, PREDICTED_DEPARTURES, FORECAST_GENERATED_AT"
                },
                {
                    "name": "DELAY_FORECAST",
                    "description": "ML forecast table predicting future delays by route and time. Helps identify potential service disruptions.",
                    "key_columns": "ROUTE_ID, ROUTE_SHORT_NAME, AGENCY, FORECAST_DATE, PREDICTED_DELAY_SECONDS, FORECAST_GENERATED_AT"
                }
            ]
        }
    ]
    
    for schema_info in schemas_info:
        add_heading_with_style(doc, f"4.{schemas_info.index(schema_info) + 1} Schema: {schema_info['name']}", 2)
        doc.add_paragraph(schema_info['description'])
        doc.add_paragraph()
        
        add_heading_with_style(doc, f"4.{schemas_info.index(schema_info) + 1}.1 Tables in {schema_info['name']} Schema", 3)
        
        for table in schema_info['tables']:
            para = doc.add_paragraph()
            run = para.add_run(f"Table: {schema_info['name']}.{table['name']}")
            run.bold = True
            doc.add_paragraph(f"Description: {table['description']}")
            doc.add_paragraph(f"Key Columns: {table['key_columns']}")
            doc.add_paragraph()
        
        doc.add_paragraph()
    
    add_table_placeholder(
        doc,
        "Complete Database Schema Diagram",
        "A comprehensive ER diagram showing all tables, their relationships, primary keys, foreign keys, and data types. Should include all tables from ANALYTICS and ML schemas with clear relationships."
    )
    
    add_page_break(doc)
    
    # 5. System Workflow
    add_heading_with_style(doc, "5. System Workflow", 1)
    
    add_heading_with_style(doc, "5.1 Data Ingestion Workflow", 2)
    doc.add_paragraph("""The system ingests data from two primary sources:""")
    
    doc.add_paragraph("""1. GTFS Static Data:""")
    doc.add_paragraph("""   - Airflow DAG (gtfs_incremental_dag) runs every 6 hours""")
    doc.add_paragraph("   - Downloads GTFS ZIP files from BART and VTA public feeds")
    doc.add_paragraph("   - Extracts and parses GTFS files (stops.txt, routes.txt, trips.txt, stop_times.txt)")
    doc.add_paragraph("   - Loads data into Snowflake LANDING tables")
    doc.add_paragraph("   - Triggers dbt transformation pipeline")
    doc.add_paragraph()
    
    doc.add_paragraph("""2. Real-Time Streaming Data:""")
    doc.add_paragraph("   - Kafka producer (transit_streaming_producer.py) runs continuously")
    doc.add_paragraph("   - Fetches real-time departures from TransitApp API (1 call per 5 minutes)")
    doc.add_paragraph("   - Publishes to Kafka topic 'transit-streaming'")
    doc.add_paragraph("   - Kafka consumer writes to ANALYTICS.LANDING_STREAMING_DEPARTURES")
    doc.add_paragraph("   - dbt models process streaming data into analytics tables")
    doc.add_paragraph()
    
    add_heading_with_style(doc, "5.2 Data Transformation Workflow", 2)
    doc.add_paragraph("""dbt models transform data through multiple layers:""")
    
    doc.add_paragraph("""1. Landing to Raw (Staging):""")
    doc.add_paragraph("   - stg_gtfs_stops: Cleans and standardizes stop data")
    doc.add_paragraph("   - stg_gtfs_routes: Cleans and standardizes route data")
    doc.add_paragraph("   - stg_gtfs_trips: Cleans and standardizes trip data")
    doc.add_paragraph("   - stg_gtfs_stop_times: Cleans and standardizes stop time data")
    doc.add_paragraph()
    
    doc.add_paragraph("""2. Streaming to Analytics:""")
    doc.add_paragraph("   - stg_streaming_departures: Processes real-time departure data")
    doc.add_paragraph()
    
    doc.add_paragraph("""3. Transform Layer:""")
    doc.add_paragraph("   - route_departures: Combines stops, routes, trips, and stop_times")
    doc.add_paragraph()
    
    doc.add_paragraph("""4. Analytics Layer:""")
    doc.add_paragraph("   - reliability_metrics: Calculates on-time performance, headway gaps, reliability scores")
    doc.add_paragraph("   - demand_metrics: Calculates demand patterns and peak periods")
    doc.add_paragraph("   - crowding_metrics: Calculates capacity utilization")
    doc.add_paragraph("   - revenue_metrics: Calculates estimated revenue")
    doc.add_paragraph("   - decision_support: Combines metrics for actionable insights")
    doc.add_paragraph()
    
    doc.add_paragraph("""5. ML Layer:""")
    doc.add_paragraph("   - demand_forecast: Predicts future demand using Snowflake ML")
    doc.add_paragraph("   - delay_forecast: Predicts future delays using Snowflake ML")
    doc.add_paragraph()
    
    add_heading_with_style(doc, "5.3 Frontend-Backend Workflow", 2)
    doc.add_paragraph("""1. User accesses React frontend (http://localhost:3000)""")
    doc.add_paragraph("2. Frontend makes API calls to FastAPI backend (http://localhost:8000/api)")
    doc.add_paragraph("3. Backend queries Snowflake for requested data")
    doc.add_paragraph("4. Backend formats and returns JSON response")
    doc.add_paragraph("5. Frontend renders data in charts, tables, and maps")
    doc.add_paragraph("6. User can interact with visualizations and request AI insights")
    doc.add_paragraph()
    
    add_image_placeholder(
        doc,
        "Complete System Workflow Diagram",
        "A detailed workflow diagram showing the complete data flow from ingestion through transformation to presentation, including all DAGs, dbt models, and API endpoints.",
        "Create a workflow diagram showing: 1) Data sources at top, 2) Airflow DAGs orchestrating ingestion, 3) Kafka streaming pipeline, 4) dbt transformation layers (landing→raw→transform→analytics→ML), 5) Snowflake storage, 6) FastAPI backend, 7) React frontend. Show timing (every 6 hours, continuous, etc.) and data volumes."
    )
    
    add_page_break(doc)
    
    # 6. Installation and Setup
    add_heading_with_style(doc, "6. Installation and Setup Guide", 1)
    
    add_heading_with_style(doc, "6.1 Prerequisites", 2)
    prerequisites = [
        "Python 3.9 or higher",
        "Docker and Docker Compose",
        "AWS CLI configured with profile 'transit-system'",
        "Snowflake account with appropriate permissions",
        "TransitApp API key (stored in AWS Secrets Manager)",
        "Perplexity AI API key (optional, for LLM features, stored in AWS Secrets Manager)",
        "Git (for cloning repository)"
    ]
    
    for prereq in prerequisites:
        doc.add_paragraph(prereq, style='List Bullet')
    
    add_heading_with_style(doc, "6.2 AWS Secrets Manager Configuration", 2)
    doc.add_paragraph("""Configure AWS Secrets Manager with the following secrets:""")
    doc.add_paragraph()
    
    secrets = [
        {
            "name": "transit/snowflake-dev",
            "format": "JSON",
            "content": '{"account": "your-account", "user": "your-user", "password": "your-password", "warehouse": "HORNET_QUERY_WH", "database": "USER_DB_HORNET", "role": "TRAINING_ROLE"}'
        },
        {
            "name": "transit/transitapp-api-dev",
            "format": "Plaintext",
            "content": "your-transitapp-api-key"
        },
        {
            "name": "transit/perplexity-api-dev",
            "format": "Plaintext or JSON",
            "content": "your-perplexity-api-key or {\"api_key\": \"your-key\"}"
        }
    ]
    
    for secret in secrets:
        para = doc.add_paragraph()
        run = para.add_run(f"Secret Name: {secret['name']}")
        run.bold = True
        doc.add_paragraph(f"Format: {secret['format']}")
        doc.add_paragraph(f"Content: {secret['content']}")
        doc.add_paragraph()
    
    doc.add_paragraph("""Create secrets using AWS CLI:""")
    doc.add_paragraph("""aws secretsmanager create-secret --name transit/snowflake-dev --secret-string '{"account":"...","user":"...","password":"..."}'""", style='Intense Quote')
    
    add_heading_with_style(doc, "6.3 Local Development Setup", 2)
    
    setup_steps = [
        ("Clone Repository", "git clone https://github.com/ayushgawai/transit-system.git\ncd transit-system"),
        ("Create Virtual Environment", "python3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate"),
        ("Install Python Dependencies", "pip install -r requirements.txt\npip install -r api/requirements.txt\npip install -r airflow/requirements.txt"),
        ("Configure AWS CLI", "aws configure --profile transit-system\n# Enter AWS Access Key ID, Secret Access Key, region (us-west-2), output format (json)"),
        ("Configure config.yaml", "Edit config.yaml to set warehouse.type to 'snowflake' and verify database settings"),
        ("Start Docker Services", "docker-compose -f docker-compose.local.yml up -d\n# This starts Kafka, Zookeeper, and Airflow"),
        ("Initialize Airflow", "Wait for Airflow to initialize (check logs: docker-compose -f docker-compose.local.yml logs airflow-webserver)\nAccess Airflow UI: http://localhost:8080 (username: admin, password: admin)"),
        ("Run Initial GTFS Load", "In Airflow UI, trigger 'gtfs_incremental_dag' manually\nThis loads historical GTFS data"),
        ("Run dbt Models", "cd dbt/transit_dbt\ndbt debug  # Verify connection\ndbt run  # Run all models"),
        ("Start Backend API", "cd api\nuvicorn main:app --reload\n# API available at http://localhost:8000\n# Docs at http://localhost:8000/docs"),
        ("Start Frontend", "cd ui\nnpm install\nnpm run dev\n# Frontend available at http://localhost:3000")
    ]
    
    for step_num, (step_name, instructions) in enumerate(setup_steps, 1):
        para = doc.add_paragraph()
        run = para.add_run(f"Step {step_num}: {step_name}")
        run.bold = True
        doc.add_paragraph(instructions, style='Intense Quote')
        doc.add_paragraph()
    
    add_heading_with_style(doc, "6.4 Verification Steps", 2)
    verification_steps = [
        "Verify Snowflake connection: Check that dbt debug succeeds",
        "Verify Airflow DAGs: Check that all DAGs appear in Airflow UI without errors",
        "Verify Kafka: Check that Kafka and Zookeeper containers are running",
        "Verify Backend API: Visit http://localhost:8000/docs and test endpoints",
        "Verify Frontend: Visit http://localhost:3000 and check that dashboard loads",
        "Verify Data: Check Snowflake tables to ensure data is loaded",
        "Verify Streaming: Start streaming producer and check that data appears in LANDING_STREAMING_DEPARTURES"
    ]
    
    for step in verification_steps:
        doc.add_paragraph(step, style='List Bullet')
    
    add_page_break(doc)
    
    # 7. Directory Structure
    add_heading_with_style(doc, "7. Project Directory Structure", 1)
    
    doc.add_paragraph("""The project follows a modular structure with clear separation of concerns:""")
    doc.add_paragraph()
    
    directory_structure = """
transit-system/
├── airflow/                    # Apache Airflow configuration
│   ├── dags/                   # Airflow DAG definitions
│   │   ├── gtfs_incremental_dag.py
│   │   ├── streaming_dag.py
│   │   └── ml_forecast_dag.py
│   ├── Dockerfile              # Airflow container definition
│   └── requirements.txt        # Python dependencies for Airflow
├── api/                        # FastAPI backend
│   ├── main.py                 # Main API application
│   ├── llm/                    # LLM integration
│   │   ├── chat_handler.py
│   │   ├── perplexity_client.py
│   │   └── schema_context.py
│   ├── warehouse_connection.py  # Database connection management
│   ├── Dockerfile
│   └── requirements.txt
├── config/                     # Configuration management
│   ├── warehouse_config.py     # Warehouse configuration loader
│   └── master_config.yaml      # Master configuration file
├── config.yaml                 # Main configuration file
├── dbt/                        # dbt project
│   └── transit_dbt/
│       ├── dbt_project.yml     # dbt project configuration
│       ├── profiles.yml        # Database connection profiles
│       └── models/             # dbt SQL models
│           ├── landing_to_raw/ # Landing to staging models
│           ├── streaming_to_analytics/ # Streaming models
│           ├── transform/     # Intermediate transformations
│           ├── analytics/      # Analytics marts
│           └── ml_forecasts/   # ML forecast models
├── ingestion/                  # Data ingestion scripts
│   ├── fetch_gtfs_incremental.py  # GTFS data extraction
│   ├── transit_streaming_producer.py  # Kafka producer
│   └── kafka_consumer_to_landing.py  # Kafka consumer
├── ui/                         # React frontend
│   ├── src/
│   │   ├── pages/              # React page components
│   │   │   ├── Dashboard.tsx
│   │   │   ├── Routes.tsx
│   │   │   ├── Analytics.tsx
│   │   │   ├── MapView.tsx
│   │   │   ├── Forecasts.tsx
│   │   │   └── Admin.tsx
│   │   ├── components/         # Reusable components
│   │   └── utils/              # Utility functions
│   ├── package.json
│   └── Dockerfile
├── docker-compose.local.yml    # Docker Compose for local development
├── README.md                   # Project README
└── requirements.txt            # Root Python dependencies
"""
    
    doc.add_paragraph(directory_structure, style='Intense Quote')
    
    add_heading_with_style(doc, "7.1 Key Directories Explained", 2)
    
    key_dirs = [
        ("airflow/dags", "Contains all Airflow DAG definitions. Each DAG orchestrates a specific data pipeline workflow."),
        ("api", "FastAPI backend application. Handles all API endpoints, database queries, and business logic."),
        ("dbt/transit_dbt/models", "Contains all dbt SQL models organized by layer (landing, staging, transform, analytics, ML)."),
        ("ingestion", "Contains Python scripts for data extraction from external sources (GTFS feeds, TransitApp API)."),
        ("ui/src", "React frontend source code. Pages contain main views, components contain reusable UI elements."),
        ("config", "Configuration files and loaders. warehouse_config.py handles AWS Secrets Manager integration.")
    ]
    
    for dir_name, description in key_dirs:
        para = doc.add_paragraph()
        run = para.add_run(f"{dir_name}: ")
        run.bold = True
        para.add_run(description)
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 8. Screenshots and Visualizations
    add_heading_with_style(doc, "8. System Screenshots and Visualizations", 1)
    
    screenshots = [
        {
            "title": "Dashboard Overview",
            "description": "Main dashboard showing key performance indicators, route health overview, and real-time metrics.",
            "location": "ui/src/pages/Dashboard.tsx"
        },
        {
            "title": "Routes Page",
            "description": "Detailed route performance view showing on-time performance, reliability scores, and route-specific metrics.",
            "location": "ui/src/pages/Routes.tsx"
        },
        {
            "title": "Analytics Dashboard",
            "description": "Comprehensive analytics view with hourly performance heatmaps, demand patterns, and export capabilities.",
            "location": "ui/src/pages/Analytics.tsx"
        },
        {
            "title": "Map View",
            "description": "Interactive map showing stop locations, route paths, and real-time departure information with color-coded performance indicators.",
            "location": "ui/src/pages/MapView.tsx"
        },
        {
            "title": "Forecasts Page",
            "description": "ML-powered demand and delay forecasts with visualizations showing predicted patterns.",
            "location": "ui/src/pages/Forecasts.tsx"
        },
        {
            "title": "Admin Panel",
            "description": "System administration panel showing data counts, pipeline status, DAG status, streaming status, and sample data.",
            "location": "ui/src/pages/Admin.tsx"
        },
        {
            "title": "Airflow UI",
            "description": "Apache Airflow web interface showing DAG status, task execution history, and workflow monitoring.",
            "location": "http://localhost:8080"
        },
        {
            "title": "API Documentation",
            "description": "FastAPI auto-generated API documentation showing all available endpoints, request/response schemas, and testing interface.",
            "location": "http://localhost:8000/docs"
        },
        {
            "title": "Snowflake Data Warehouse",
            "description": "Snowflake web interface showing database schemas, tables, and query results.",
            "location": "Snowflake Web UI"
        },
        {
            "title": "AWS Secrets Manager",
            "description": "AWS console showing configured secrets for Snowflake, TransitApp API, and Perplexity API.",
            "location": "AWS Console"
        }
    ]
    
    for i, screenshot in enumerate(screenshots, 1):
        add_image_placeholder(
            doc,
            f"Figure {i}: {screenshot['title']}",
            screenshot['description'],
            f"Screenshot of {screenshot['title']} showing {screenshot['description']}. Location: {screenshot['location']}"
        )
    
    add_page_break(doc)
    
    # 9. Usage Guide
    add_heading_with_style(doc, "9. End-to-End Usage Guide", 1)
    
    add_heading_with_style(doc, "9.1 Starting the System", 2)
    doc.add_paragraph("""To start the complete system:""")
    doc.add_paragraph()
    doc.add_paragraph("1. Start Docker services:", style='List Bullet')
    doc.add_paragraph("   docker-compose -f docker-compose.local.yml up -d", style='Intense Quote')
    doc.add_paragraph()
    doc.add_paragraph("2. Wait for Airflow to initialize (2-3 minutes)", style='List Bullet')
    doc.add_paragraph()
    doc.add_paragraph("3. Start backend API:", style='List Bullet')
    doc.add_paragraph("   cd api && uvicorn main:app --reload", style='Intense Quote')
    doc.add_paragraph()
    doc.add_paragraph("4. Start frontend:", style='List Bullet')
    doc.add_paragraph("   cd ui && npm run dev", style='Intense Quote')
    doc.add_paragraph()
    
    add_heading_with_style(doc, "9.2 Loading Initial Data", 2)
    doc.add_paragraph("1. Access Airflow UI: http://localhost:8080", style='List Bullet')
    doc.add_paragraph("2. Find 'gtfs_incremental_dag' DAG", style='List Bullet')
    doc.add_paragraph("3. Click 'Trigger DAG' to start initial data load", style='List Bullet')
    doc.add_paragraph("4. Monitor task execution in Airflow UI", style='List Bullet')
    doc.add_paragraph("5. Verify data in Snowflake: Query ANALYTICS.STG_GTFS_STOPS", style='List Bullet')
    doc.add_paragraph()
    
    add_heading_with_style(doc, "9.3 Starting Streaming Data", 2)
    doc.add_paragraph("Option 1: Via Admin Panel", style='List Bullet')
    doc.add_paragraph("   1. Access frontend: http://localhost:3000/admin", style='List Number')
    doc.add_paragraph("   2. Click 'Start Streaming' button", style='List Number')
    doc.add_paragraph("   3. Monitor streaming status in admin panel", style='List Number')
    doc.add_paragraph()
    doc.add_paragraph("Option 2: Via Command Line", style='List Bullet')
    doc.add_paragraph("   python3 ingestion/transit_streaming_producer.py", style='Intense Quote')
    doc.add_paragraph()
    
    add_heading_with_style(doc, "9.4 Running dbt Transformations", 2)
    doc.add_paragraph("1. Navigate to dbt project:", style='List Bullet')
    doc.add_paragraph("   cd dbt/transit_dbt", style='Intense Quote')
    doc.add_paragraph("2. Verify connection:", style='List Bullet')
    doc.add_paragraph("   dbt debug", style='Intense Quote')
    doc.add_paragraph("3. Run all models:", style='List Bullet')
    doc.add_paragraph("   dbt run", style='Intense Quote')
    doc.add_paragraph("4. Run specific model:", style='List Bullet')
    doc.add_paragraph("   dbt run --select reliability_metrics", style='Intense Quote')
    doc.add_paragraph()
    
    add_heading_with_style(doc, "9.5 Accessing the Frontend", 2)
    doc.add_paragraph("1. Open browser: http://localhost:3000", style='List Bullet')
    doc.add_paragraph("2. Navigate through pages:", style='List Bullet')
    doc.add_paragraph("   - Dashboard: Overview metrics", style='List Number')
    doc.add_paragraph("   - Routes: Route performance details", style='List Number')
    doc.add_paragraph("   - Analytics: Detailed analytics", style='List Number')
    doc.add_paragraph("   - Map View: Geographic visualization", style='List Number')
    doc.add_paragraph("   - Forecasts: ML predictions", style='List Number')
    doc.add_paragraph("   - Admin: System status", style='List Number')
    doc.add_paragraph("3. Use agency dropdown to filter BART/VTA data", style='List Bullet')
    doc.add_paragraph("4. Click 'i' buttons on charts for AI insights", style='List Bullet')
    doc.add_paragraph()
    
    add_page_break(doc)
    
    # 10. Analytics Tables Reference
    add_heading_with_style(doc, "10. Analytics Tables Reference for Dashboard Development", 1)
    
    doc.add_paragraph("""This section provides detailed information about analytics tables for developers creating dashboards or reports.""")
    doc.add_paragraph()
    
    analytics_tables = [
        {
            "database": "USER_DB_HORNET",
            "schema": "ANALYTICS",
            "table": "RELIABILITY_METRICS",
            "description": "Primary table for service reliability analysis. Contains on-time performance metrics, headway analysis, and reliability scores.",
            "key_metrics": [
                "ON_TIME_PCT: Percentage of on-time departures (0-100)",
                "AVG_DELAY_SECONDS: Average delay in seconds",
                "RELIABILITY_SCORE: Composite reliability score (0-100)",
                "HEADWAY_GAP_COUNT: Number of headway gaps detected",
                "TOTAL_DEPARTURES: Total number of departures in period"
            ],
            "common_queries": [
                "SELECT * FROM RELIABILITY_METRICS WHERE ROUTE_SHORT_NAME = 'YELLOW' ORDER BY DEPARTURE_DATE DESC, DEPARTURE_HOUR",
                "SELECT ROUTE_SHORT_NAME, AVG(ON_TIME_PCT) as AVG_ON_TIME FROM RELIABILITY_METRICS GROUP BY ROUTE_SHORT_NAME",
                "SELECT DEPARTURE_HOUR, AVG(RELIABILITY_SCORE) FROM RELIABILITY_METRICS GROUP BY DEPARTURE_HOUR ORDER BY DEPARTURE_HOUR"
            ],
            "use_cases": "Route performance dashboards, on-time performance reports, reliability trend analysis, peak hour analysis"
        },
        {
            "database": "USER_DB_HORNET",
            "schema": "ANALYTICS",
            "table": "DEMAND_METRICS",
            "description": "Table for demand pattern analysis. Contains departure counts by time period, peak analysis, and demand intensity scores.",
            "key_metrics": [
                "TOTAL_DEPARTURES: Total departures for the day",
                "AM_PEAK_DEPARTURES: Departures during morning peak (7-9 AM)",
                "PM_PEAK_DEPARTURES: Departures during evening peak (4-7 PM)",
                "DEMAND_INTENSITY_SCORE: Demand intensity score (0-100)",
                "PEAK_PCT: Percentage of departures during peak periods"
            ],
            "common_queries": [
                "SELECT STOP_NAME, TOTAL_DEPARTURES, AM_PEAK_DEPARTURES, PM_PEAK_DEPARTURES FROM DEMAND_METRICS ORDER BY TOTAL_DEPARTURES DESC",
                "SELECT DEPARTURE_DAY_NAME, AVG(TOTAL_DEPARTURES) FROM DEMAND_METRICS GROUP BY DEPARTURE_DAY_NAME",
                "SELECT * FROM DEMAND_METRICS WHERE DEMAND_INTENSITY_SCORE > 80 ORDER BY TOTAL_DEPARTURES DESC"
            ],
            "use_cases": "Demand forecasting, peak period analysis, stop utilization reports, service planning"
        },
        {
            "database": "USER_DB_HORNET",
            "schema": "ANALYTICS",
            "table": "LANDING_STREAMING_DEPARTURES",
            "description": "Real-time streaming departures table. Contains live departure data with delays and real-time status from TransitApp API.",
            "key_metrics": [
                "DELAY_SECONDS: Delay in seconds (NULL if not available)",
                "IS_REAL_TIME: Boolean indicating if data is real-time",
                "SCHEDULED_DEPARTURE_TIME: Scheduled departure time (epoch)",
                "DEPARTURE_TIME: Actual/predicted departure time (epoch)",
                "CONSUMED_AT: Timestamp when record was ingested"
            ],
            "common_queries": [
                "SELECT * FROM LANDING_STREAMING_DEPARTURES WHERE DELAY_SECONDS > 300 ORDER BY CONSUMED_AT DESC LIMIT 100",
                "SELECT ROUTE_SHORT_NAME, AVG(DELAY_SECONDS) as AVG_DELAY FROM LANDING_STREAMING_DEPARTURES WHERE DELAY_SECONDS IS NOT NULL GROUP BY ROUTE_SHORT_NAME",
                "SELECT CITY, COUNT(*) as TOTAL_DEPARTURES FROM LANDING_STREAMING_DEPARTURES WHERE CONSUMED_AT > DATEADD(hour, -1, CURRENT_TIMESTAMP()) GROUP BY CITY"
            ],
            "use_cases": "Real-time dashboards, live delay monitoring, current service status, streaming data analysis"
        },
        {
            "database": "USER_DB_HORNET",
            "schema": "ML",
            "table": "DEMAND_FORECAST",
            "description": "ML forecast table predicting future demand by route. Generated using Snowflake ML based on historical patterns.",
            "key_metrics": [
                "PREDICTED_DEPARTURES: Predicted number of departures for forecast date",
                "FORECAST_DATE: Date for which forecast is made",
                "FORECAST_GENERATED_AT: Timestamp when forecast was generated"
            ],
            "common_queries": [
                "SELECT * FROM DEMAND_FORECAST WHERE FORECAST_DATE >= CURRENT_DATE() ORDER BY FORECAST_DATE, ROUTE_SHORT_NAME",
                "SELECT ROUTE_SHORT_NAME, SUM(PREDICTED_DEPARTURES) as TOTAL_PREDICTED FROM DEMAND_FORECAST WHERE FORECAST_DATE BETWEEN CURRENT_DATE() AND DATEADD(day, 7, CURRENT_DATE()) GROUP BY ROUTE_SHORT_NAME"
            ],
            "use_cases": "Demand forecasting dashboards, capacity planning, resource allocation planning"
        },
        {
            "database": "USER_DB_HORNET",
            "schema": "ANALYTICS",
            "table": "STG_GTFS_ROUTES",
            "description": "Staging table for GTFS routes. Contains route information including names, agency, and metadata.",
            "key_metrics": [
                "ROUTE_ID: Unique route identifier",
                "ROUTE_SHORT_NAME: Short route name (e.g., 'YELLOW')",
                "ROUTE_LONG_NAME: Full route name (e.g., 'Antioch to SF Int'l Airport SFO')",
                "AGENCY: Transit agency (BART, VTA, etc.)"
            ],
            "common_queries": [
                "SELECT DISTINCT AGENCY FROM STG_GTFS_ROUTES",
                "SELECT ROUTE_SHORT_NAME, ROUTE_LONG_NAME FROM STG_GTFS_ROUTES WHERE AGENCY = 'BART'",
                "SELECT COUNT(*) as TOTAL_ROUTES, AGENCY FROM STG_GTFS_ROUTES GROUP BY AGENCY"
            ],
            "use_cases": "Route listing, agency filtering, route metadata lookup"
        },
        {
            "database": "USER_DB_HORNET",
            "schema": "ANALYTICS",
            "table": "STG_GTFS_STOPS",
            "description": "Staging table for GTFS stops. Contains stop locations, names, and geographic coordinates.",
            "key_metrics": [
                "STOP_ID: Unique stop identifier",
                "STOP_NAME: Stop name",
                "STOP_LAT: Latitude coordinate",
                "STOP_LON: Longitude coordinate",
                "AGENCY: Transit agency"
            ],
            "common_queries": [
                "SELECT STOP_NAME, STOP_LAT, STOP_LON FROM STG_GTFS_STOPS WHERE AGENCY = 'VTA' LIMIT 100",
                "SELECT COUNT(*) as TOTAL_STOPS, AGENCY FROM STG_GTFS_STOPS GROUP BY AGENCY",
                "SELECT * FROM STG_GTFS_STOPS WHERE STOP_LAT BETWEEN 37.3 AND 37.4 AND STOP_LON BETWEEN -122.5 AND -122.4"
            ],
            "use_cases": "Map visualizations, stop lookup, geographic analysis, route planning"
        }
    ]
    
    for i, table_info in enumerate(analytics_tables, 1):
        add_heading_with_style(doc, f"10.{i} {table_info['table']}", 2)
        
        para = doc.add_paragraph()
        run = para.add_run("Full Table Name: ")
        run.bold = True
        para.add_run(f"{table_info['database']}.{table_info['schema']}.{table_info['table']}")
        doc.add_paragraph()
        
        doc.add_paragraph(f"Description: {table_info['description']}")
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Key Metrics/Columns: ")
        run.bold = True
        doc.add_paragraph()
        for metric in table_info['key_metrics']:
            doc.add_paragraph(metric, style='List Bullet')
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Common Queries: ")
        run.bold = True
        doc.add_paragraph()
        for query in table_info['common_queries']:
            doc.add_paragraph(query, style='Intense Quote')
        doc.add_paragraph()
        
        para = doc.add_paragraph()
        run = para.add_run("Use Cases: ")
        run.bold = True
        para.add_run(table_info['use_cases'])
        doc.add_paragraph()
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 11. Conclusion
    add_heading_with_style(doc, "11. Conclusion", 1)
    
    doc.add_paragraph("""The Transit Service Reliability & Demand Planning System represents a comprehensive, 
    production-capable solution for metropolitan transit operators. By integrating modern data engineering 
    technologies including Apache Airflow, dbt, Snowflake, AWS services, and React, the system provides a 
    complete end-to-end platform for transit data management and analytics.""")
    doc.add_paragraph()
    
    doc.add_paragraph("""Key achievements of this system include:""")
    achievements = [
        "Unified data pipeline processing both static GTFS feeds and real-time streaming data",
        "Scalable architecture capable of handling large volumes of transit data",
        "Real-time analytics providing actionable insights to operations teams",
        "ML-powered forecasting for demand prediction and delay forecasting",
        "Intuitive frontend dashboard enabling non-technical users to access insights",
        "Production-ready design with proper error handling, monitoring, and security",
        "Comprehensive documentation enabling easy setup and maintenance"
    ]
    
    for achievement in achievements:
        doc.add_paragraph(achievement, style='List Bullet')
    
    doc.add_paragraph()
    doc.add_paragraph("""The system demonstrates best practices in data engineering, including proper data 
    modeling, incremental processing, version control, and containerization. It provides a solid foundation 
    for transit operators to improve service reliability, optimize resource allocation, and enhance passenger 
    experience through data-driven decision-making.""")
    
    add_page_break(doc)
    
    # 12. Future Prospects
    add_heading_with_style(doc, "12. Future Prospects and Enhancements", 1)
    
    future_enhancements = [
        {
            "title": "Advanced ML Models",
            "description": "Implement more sophisticated ML models for demand forecasting, including time series models (ARIMA, Prophet), deep learning models (LSTM), and ensemble methods. Integrate external factors such as weather, events, and holidays."
        },
        {
            "title": "Real-Time Alerting",
            "description": "Implement real-time alerting system that notifies operations teams when service issues are detected, such as significant delays, route disruptions, or capacity issues."
        },
        {
            "title": "Mobile Application",
            "description": "Develop mobile applications (iOS/Android) for operations teams to access dashboards and receive alerts on-the-go."
        },
        {
            "title": "Multi-Agency Support",
            "description": "Extend support for additional transit agencies beyond BART and VTA, including MUNI, Caltrain, and other Bay Area transit providers."
        },
        {
            "title": "Advanced Analytics",
            "description": "Add advanced analytics features including route optimization recommendations, capacity planning tools, and cost-benefit analysis for service changes."
        },
        {
            "title": "Integration with External Systems",
            "description": "Integrate with external systems such as passenger counting systems, fare collection systems, and maintenance management systems for comprehensive operations visibility."
        },
        {
            "title": "Automated Reporting",
            "description": "Implement automated report generation and distribution, including daily/weekly/monthly performance reports sent to stakeholders."
        },
        {
            "title": "Predictive Maintenance",
            "description": "Extend ML capabilities to predict vehicle maintenance needs and optimize maintenance schedules based on usage patterns."
        },
        {
            "title": "Passenger-Facing Features",
            "description": "Develop passenger-facing features such as real-time arrival predictions, service disruption notifications, and trip planning integration."
        },
        {
            "title": "Cloud-Native Deployment",
            "description": "Deploy to production cloud environments (AWS EKS, ECS) with auto-scaling, load balancing, and high availability configurations."
        }
    ]
    
    for i, enhancement in enumerate(future_enhancements, 1):
        para = doc.add_paragraph()
        run = para.add_run(f"12.{i} {enhancement['title']}")
        run.bold = True
        doc.add_paragraph(enhancement['description'])
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 13. References
    add_heading_with_style(doc, "13. References and Sources", 1)
    
    references = [
        ("GTFS Specification", "https://gtfs.org/reference/static", "Official GTFS (General Transit Feed Specification) documentation"),
        ("Apache Airflow Documentation", "https://airflow.apache.org/docs/", "Official Apache Airflow documentation and guides"),
        ("dbt Documentation", "https://docs.getdbt.com/", "Official dbt (data build tool) documentation"),
        ("Snowflake Documentation", "https://docs.snowflake.com/", "Official Snowflake data warehouse documentation"),
        ("Snowflake ML Documentation", "https://docs.snowflake.com/en/user-guide/snowflake-ml", "Snowflake ML functions and forecasting"),
        ("FastAPI Documentation", "https://fastapi.tiangolo.com/", "FastAPI framework documentation"),
        ("React Documentation", "https://react.dev/", "React JavaScript library documentation"),
        ("TransitApp API Documentation", "https://api-doc.transitapp.com/v3.html", "TransitApp API v3 documentation"),
        ("AWS Secrets Manager", "https://docs.aws.amazon.com/secretsmanager/", "AWS Secrets Manager documentation"),
        ("Apache Kafka Documentation", "https://kafka.apache.org/documentation/", "Apache Kafka documentation"),
        ("Docker Documentation", "https://docs.docker.com/", "Docker containerization platform documentation"),
        ("Chart.js Documentation", "https://www.chartjs.org/docs/", "Chart.js data visualization library"),
        ("Leaflet Documentation", "https://leafletjs.com/", "Leaflet map library documentation"),
        ("Perplexity AI", "https://www.perplexity.ai/", "Perplexity AI API documentation"),
        ("IEEE Software Engineering Standards", "https://www.ieee.org/", "IEEE standards for software documentation")
    ]
    
    for i, (title, url, description) in enumerate(references, 1):
        para = doc.add_paragraph()
        run = para.add_run(f"[{i}] ")
        run.bold = True
        para.add_run(f"{title}. {description}. Available at: {url}")
        doc.add_paragraph()
    
    add_page_break(doc)
    
    # 14. Appendix
    add_heading_with_style(doc, "14. Appendix", 1)
    
    add_heading_with_style(doc, "14.1 GitHub Repository", 2)
    para = doc.add_paragraph()
    run = para.add_run("Repository URL: ")
    run.bold = True
    run = para.add_run("https://github.com/ayushgawai/transit-system")
    run.font.color.rgb = RGBColor(0, 0, 255)
    
    doc.add_paragraph()
    doc.add_paragraph("""The complete source code, configuration files, and additional documentation are available 
    in the GitHub repository. The repository includes:""")
    
    repo_contents = [
        "Complete source code for all components",
        "Configuration files and setup scripts",
        "Docker Compose files for local development",
        "dbt models and SQL transformations",
        "Airflow DAG definitions",
        "Frontend React application",
        "API backend code",
        "Documentation and README files"
    ]
    
    for content in repo_contents:
        doc.add_paragraph(content, style='List Bullet')
    
    add_heading_with_style(doc, "14.2 Contact Information", 2)
    doc.add_paragraph("Developer: Ayush Gawai")
    doc.add_paragraph("Institution: San Jose State University")
    doc.add_paragraph("Project: MSDA Capstone Project")
    doc.add_paragraph("Year: 2025")
    
    add_heading_with_style(doc, "14.3 License", 2)
    doc.add_paragraph("""This project is developed as part of an academic capstone project. Please refer to the 
    LICENSE file in the GitHub repository for licensing information.""")
    
    add_heading_with_style(doc, "14.4 Troubleshooting Common Issues", 2)
    
    troubleshooting = [
        {
            "issue": "Airflow DAGs not appearing",
            "solution": "Check Airflow logs: docker-compose -f docker-compose.local.yml logs airflow-scheduler. Ensure DAG files are in airflow/dags directory and have no syntax errors."
        },
        {
            "issue": "Snowflake connection errors",
            "solution": "Verify AWS Secrets Manager has correct credentials. Check config.yaml warehouse settings. Test connection with: dbt debug"
        },
        {
            "issue": "Frontend shows 'Failed to fetch'",
            "solution": "Ensure backend API is running on http://localhost:8000. Check CORS settings in api/main.py. Verify API endpoints in browser developer console."
        },
        {
            "issue": "No data in dashboard",
            "solution": "Verify GTFS data has been loaded (check Airflow DAG execution). Run dbt models to populate analytics tables. Check Snowflake tables directly."
        },
        {
            "issue": "Streaming data not appearing",
            "solution": "Check if streaming producer is running. Verify TransitApp API key in AWS Secrets Manager. Check Kafka topics and consumer status."
        },
        {
            "issue": "dbt models failing",
            "solution": "Check dbt debug output. Verify Snowflake connection. Check model SQL for syntax errors. Review dbt logs in dbt/transit_dbt/logs/"
        }
    ]
    
    for i, item in enumerate(troubleshooting, 1):
        para = doc.add_paragraph()
        run = para.add_run(f"Issue {i}: {item['issue']}")
        run.bold = True
        doc.add_paragraph(f"Solution: {item['solution']}")
        doc.add_paragraph()
    
    # Save document
    output_path = "Transit_System_Documentation.docx"
    doc.save(output_path)
    print(f"✅ Documentation created: {output_path}")
    return output_path

if __name__ == "__main__":
    create_documentation()

